{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dll_hw__5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQlN5rP6abZTbEvaHRUA+e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dviva1972/denvlaiva/blob/master/dll_hw__5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gnr5b_WvX7m"
      },
      "source": [
        "## DLL\n",
        "\n",
        "## Домашняя работа 5  |  RNN\n",
        "## Иванов Денис"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwq0jG2ZuDrZ"
      },
      "source": [
        "## Задание 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR0L_WYruLAs"
      },
      "source": [
        "1. Напишите алгоритм шифра цезаря для генерации выборки \n",
        "\n",
        "2.  Создайте и обучите нейронную сеть (вход - зашифрованная фраза, выход - дешифрованная фраза)\n",
        "\n",
        "3. Проверьте качество модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Rq-LP3YuI2w"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAnYff42v4TQ"
      },
      "source": [
        "1.Алгоритм шифра Цезаря "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeaiOWUIuPc5"
      },
      "source": [
        "def Сaesar(string, num):\n",
        "    output = ''\n",
        "    for c in string:\n",
        "        if c.isalpha():\n",
        "            new_num = ord(c) + num\n",
        "            if new_num > ord('z'):\n",
        "                new_num -= 26\n",
        "            output += chr(new_num)\n",
        "        else:\n",
        "            output += c\n",
        "    return output"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8K4bw1OBuVUG",
        "outputId": "10687c7c-5d6c-46f0-c6b0-52a5649cf7a1"
      },
      "source": [
        "Сaesar('trew', 2)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'vtgy'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-Wm73XEwZVd"
      },
      "source": [
        "Сгенерим данные на базе изречений симпсонов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AObYk7DbovUs"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "from google.colab import drive "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9--cCzKKGzY"
      },
      "source": [
        "import copy"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLDPN3VXJsLJ"
      },
      "source": [
        "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkKa2S1so4jE",
        "outputId": "a82c6ee6-4e94-42f6-b706-9f8d8d7dd6ba"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "zE4mhjfquf-m",
        "outputId": "f976a381-331c-4a50-988d-46adf83bc4da"
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/DLL/simpsons/data.csv', encoding='utf-8-sig').iloc[:,[-2]]\n",
        "df = df.dropna(subset=['normalized_text'])\n",
        "\n",
        "df['shift']   = [random.randint(1, 10) for i in range(len(df))]\n",
        "df['text_in'] = [' '.join(re.findall('[\\w]+', i)) for i in df['normalized_text']]\n",
        "df['text_out']= df.loc[:, ['text_in', 'shift']].apply(\n",
        "                        lambda row: Сaesar(row['text_in'], row['shift']), axis=1)\n",
        "\n",
        "df = df.iloc[:,1:]\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>shift</th>\n",
              "      <th>text_in</th>\n",
              "      <th>text_out</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>maggie look whats that</td>\n",
              "      <td>rfllnj qttp bmfyx ymfy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>lee mur lee mur</td>\n",
              "      <td>ohh pxu ohh pxu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>zee boo zee boo</td>\n",
              "      <td>chh err chh err</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>im trying to teach maggie that nature doesnt e...</td>\n",
              "      <td>ko vtakpi vq vgcej ociikg vjcv pcvwtg fqgupv g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>its like an ox only it has a hump and a dewlap...</td>\n",
              "      <td>qba tqsm iv wf wvtg qb pia i pcux ivl i lmetix...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   shift  ...                                           text_out\n",
              "0      5  ...                             rfllnj qttp bmfyx ymfy\n",
              "1      3  ...                                    ohh pxu ohh pxu\n",
              "2      3  ...                                    chh err chh err\n",
              "3      2  ...  ko vtakpi vq vgcej ociikg vjcv pcvwtg fqgupv g...\n",
              "4      8  ...  qba tqsm iv wf wvtg qb pia i pcux ivl i lmetix...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ewjerm-mbyco",
        "outputId": "6b55170d-fcdc-4098-e37a-800c838b8ebc"
      },
      "source": [
        "[[c for c in ph] for ph in df.text_in if type(ph) is not str]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAhwTuOUbUfs"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQd6ZaqzavkE"
      },
      "source": [
        "train, test = train_test_split(df, test_size=0.2)\n",
        "train_text  = train['text_out' ].tolist()\n",
        "train_label = train['text_in'].tolist()\n",
        "test_text   = test[ 'text_out' ].tolist()\n",
        "test_label  = test[ 'text_in'].tolist()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gsSpu3SOVsa"
      },
      "source": [
        "INDEX_TO_CHAR = ['none'] + [w for w in set('abcdefghijklmnopqrstuvwxyz ')]\n",
        "\n",
        "dict_in  = {w: i for i, w in enumerate(INDEX_TO_CHAR)}\n",
        "dict_out = {v:k for k, v in dict_in.items()}\n",
        "\n",
        "MAX_LEN       = 20\n",
        "\n",
        "def convert_to_torch(text):\n",
        "    output = torch.zeros((len(text), MAX_LEN), dtype=int)\n",
        "    for i in range(len(text)):\n",
        "        for j, w in enumerate(text[i]):\n",
        "            if j >= MAX_LEN:\n",
        "                break\n",
        "            output[i, j] = dict_in.get(w, dict_in['none'])\n",
        "\n",
        "    return output\n",
        "\n",
        "X_train= convert_to_torch(train_text)\n",
        "Y_train= convert_to_torch(train_label)\n",
        "X_test = convert_to_torch(test_text)\n",
        "Y_test = convert_to_torch(test_label)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm2UTy-eavqU"
      },
      "source": [
        "class RNN_Network(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNN_Network, self).__init__()\n",
        "        self.embeddings = torch.nn.Embedding(len(dict_in), 28)\n",
        "        self.rnn = torch.nn.RNN(28, 256, batch_first=True)\n",
        "        self.linear = torch.nn.Linear(256, 28)\n",
        "\n",
        "    def forward(self, sentences, state=None):\n",
        "        embds = self.embeddings(sentences)\n",
        "        out, new_state = self.rnn(embds, state)\n",
        "        result = self.linear(out)\n",
        "        return result, new_state"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8zf6Spu8kxd"
      },
      "source": [
        "model = RNN_Network()\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
        "epochs = 20"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdSOS6x7WEFD",
        "outputId": "eb2a0ac0-5e52-4477-c7e0-56b4ffdd09a9"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    train_loss = 0.\n",
        "    train_passed = 0\n",
        "    test_loss = 0\n",
        "    test_passed = 0\n",
        "\n",
        "    for i in range(int(len(X_train) / 100)):\n",
        "        X_batch = X_train[i * 100:(i + 1) * 100]\n",
        "        Y_batch = Y_train[i * 100:(i + 1) * 100].flatten()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        answers, _ = model.forward(X_batch)\n",
        "        answers = answers.view(-1, len(dict_in))\n",
        "        loss = criterion(answers, Y_batch)\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_passed += 1\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        answers, _ = model.forward(X_test)\n",
        "        answers = answers.view(-1, len(dict_in))\n",
        "        loss = criterion(answers, Y_test.flatten())\n",
        "        test_loss += loss.item()\n",
        "        test_passed += 1\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}. Time: {time.time() - start:.3f}, Train loss: {train_loss / train_passed:.3f}, Test loss: {test_loss / test_passed:.6f}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1. Time: 2.104, Train loss: 1.691, Test loss: 1.308471\n",
            "Epoch 2. Time: 1.717, Train loss: 1.251, Test loss: 1.205643\n",
            "Epoch 3. Time: 1.742, Train loss: 1.160, Test loss: 1.115506\n",
            "Epoch 4. Time: 1.743, Train loss: 1.040, Test loss: 0.970439\n",
            "Epoch 5. Time: 1.751, Train loss: 0.879, Test loss: 0.828048\n",
            "Epoch 6. Time: 1.709, Train loss: 0.729, Test loss: 0.701996\n",
            "Epoch 7. Time: 1.689, Train loss: 0.627, Test loss: 0.614614\n",
            "Epoch 8. Time: 1.731, Train loss: 0.555, Test loss: 0.554984\n",
            "Epoch 9. Time: 1.724, Train loss: 0.484, Test loss: 0.506925\n",
            "Epoch 10. Time: 1.814, Train loss: 0.424, Test loss: 0.463546\n",
            "Epoch 11. Time: 1.806, Train loss: 0.380, Test loss: 0.429090\n",
            "Epoch 12. Time: 1.824, Train loss: 0.350, Test loss: 0.417009\n",
            "Epoch 13. Time: 1.749, Train loss: 0.330, Test loss: 0.400723\n",
            "Epoch 14. Time: 1.742, Train loss: 0.311, Test loss: 0.381552\n",
            "Epoch 15. Time: 1.725, Train loss: 0.289, Test loss: 0.355932\n",
            "Epoch 16. Time: 1.723, Train loss: 0.260, Test loss: 0.354104\n",
            "Epoch 17. Time: 1.668, Train loss: 0.243, Test loss: 0.340754\n",
            "Epoch 18. Time: 1.669, Train loss: 0.228, Test loss: 0.323673\n",
            "Epoch 19. Time: 1.645, Train loss: 0.216, Test loss: 0.329973\n",
            "Epoch 20. Time: 1.664, Train loss: 0.206, Test loss: 0.318408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU5xOobu1nt3"
      },
      "source": [
        "###Задание 2.\n",
        "\n",
        "На основе лекционного ноутбука.\n",
        "\n",
        "а) построить RNN-ячейку на основе полносвязных слоев\n",
        "\n",
        "б) применить построенную ячейку для генерации текста с выражениями героев сериала “Симпсоны”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpwH3UmfhHJb"
      },
      "source": [
        "CHAR_TO_INDEX = {w: i for i, w in enumerate(INDEX_TO_CHAR)}\n",
        "MAX_LEN = 50\n",
        "n_classes = len(INDEX_TO_CHAR)\n",
        "X = convert_to_torch(df['text_in'].tolist())"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwuCu3Pweaqj"
      },
      "source": [
        "class Network(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.embed = torch.nn.Embedding(len(CHAR_TO_INDEX), 28)\n",
        "        self.rnn = torch.nn.RNN(28, 812, batch_first=True)\n",
        "        self.linear = torch.nn.Linear(812, len(INDEX_TO_CHAR))\n",
        "        \n",
        "    def forward(self, sentences, state=None):\n",
        "        embed = self.embed(sentences)\n",
        "        o, s = self.rnn(embed)\n",
        "        out = self.linear(o)\n",
        "        return out"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm007vK-cKO3"
      },
      "source": [
        "def generate_sentence(txt_list):\n",
        "\n",
        "    sentence =  [c for c in txt_list]\n",
        "    x = torch.zeros((1, len(sentence)), dtype=int).to(dev)\n",
        "    \n",
        "    for j,w in enumerate(sentence):\n",
        "        if j>=MAX_LEN:\n",
        "            break\n",
        "        x[0, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX['none'])\n",
        "        \n",
        "    for i in range(MAX_LEN):\n",
        "        o = model(x)\n",
        "        w = torch.argmax(o[-1, -1, :], keepdim=True)\n",
        "        x = torch.cat([x, w.unsqueeze(0).unsqueeze(1)], axis=1)\n",
        "        ww = INDEX_TO_CHAR[w]\n",
        "        if ww == 'none':\n",
        "            break\n",
        "\n",
        "        sentence.append(ww)\n",
        "\n",
        "    return ''.join(sentence)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSKTInTfeBZY"
      },
      "source": [
        "model     = Network().to(dev)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=.0001)\n",
        "\n",
        "phrase = 'youre a great mom you were always '"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bqFARFzcKSS",
        "outputId": "2351f95b-fbe5-4214-8d96-5ef04749bc11"
      },
      "source": [
        "for ep in range(200):\n",
        "    start = time.time()\n",
        "    train_loss = 0.\n",
        "    train_passed = 0\n",
        "\n",
        "    for i in range(int(len(X) / 100)):\n",
        "        batch = X[i * 100:(i + 1) * 100] \n",
        "        X_batch = batch[:, :-1].to(dev)\n",
        "        Y_batch = batch[:, 1:].flatten().to(dev) \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        answers = model(X_batch)\n",
        "        answers = answers.view(-1, len(INDEX_TO_CHAR))\n",
        "        loss = criterion(answers, Y_batch).to(dev)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_passed += 1     \n",
        "\n",
        "    if ep%5 == 0: \n",
        "        print(\"\\nEpoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))\n",
        "        s = generate_sentence(phrase)\n",
        "        print(s)\n",
        "    else:\n",
        "        print(f\"\\rEpoch {ep}, loss: {train_loss / train_passed:.3f}\", end='') "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 0. Time: 1.500, Train loss: 2.104\n",
            "youre a great mom you were always to to to to to to to to to to to to to to to to to\n",
            "Epoch 4, loss: 1.558\n",
            "Epoch 5. Time: 1.243, Train loss: 1.524\n",
            "youre a great mom you were always are the the the the the the the the the the the th\n",
            "Epoch 9, loss: 1.424\n",
            "Epoch 10. Time: 1.257, Train loss: 1.402\n",
            "youre a great mom you were always the some his the some hing the some bert in the so\n",
            "Epoch 14, loss: 1.328\n",
            "Epoch 15. Time: 1.268, Train loss: 1.312\n",
            "youre a great mom you were always the bart the some his the bart the some his the ba\n",
            "Epoch 19, loss: 1.253\n",
            "Epoch 20. Time: 1.278, Train loss: 1.240\n",
            "youre a great mom you were always the bart was the berter the some hive to see the b\n",
            "Epoch 24, loss: 1.191\n",
            "Epoch 25. Time: 1.282, Train loss: 1.181\n",
            "youre a great mom you were always the best simeshing the serest is the bart was the \n",
            "Epoch 29, loss: 1.141\n",
            "Epoch 30. Time: 1.282, Train loss: 1.132\n",
            "youre a great mom you were always the best simpson and the serest is a stople someth\n",
            "Epoch 34, loss: 1.098\n",
            "Epoch 35. Time: 1.292, Train loss: 1.090\n",
            "youre a great mom you were always the best simpson and i was a story of the bart was\n",
            "Epoch 39, loss: 1.062\n",
            "Epoch 40. Time: 1.296, Train loss: 1.055\n",
            "youre a great mom you were always the best scimes are you so mad i dont know what i \n",
            "Epoch 44, loss: 1.031\n",
            "Epoch 45. Time: 1.302, Train loss: 1.025\n",
            "youre a great mom you were always the best science the bart was a bart the best scho\n",
            "Epoch 49, loss: 1.003\n",
            "Epoch 50. Time: 1.312, Train loss: 0.998\n",
            "youre a great mom you were always the best science to the back to the back to the ba\n",
            "Epoch 54, loss: 0.978\n",
            "Epoch 55. Time: 1.316, Train loss: 0.974\n",
            "youre a great mom you were always the best science to the back to the back to the ba\n",
            "Epoch 59, loss: 0.956\n",
            "Epoch 60. Time: 1.323, Train loss: 0.952\n",
            "youre a great mom you were always the best science to the back to the back to the ba\n",
            "Epoch 64, loss: 0.935\n",
            "Epoch 65. Time: 1.328, Train loss: 0.931\n",
            "youre a great mom you were always think its so sweet the carnot to me\n",
            "Epoch 69, loss: 0.915\n",
            "Epoch 70. Time: 1.351, Train loss: 0.912\n",
            "youre a great mom you were always think its so sweet the carnot to me\n",
            "Epoch 74, loss: 0.897\n",
            "Epoch 75. Time: 1.345, Train loss: 0.893\n",
            "youre a great mom you were always think its so sweet i cant believe it was a bart to\n",
            "Epoch 79, loss: 0.879\n",
            "Epoch 80. Time: 1.349, Train loss: 0.876\n",
            "youre a great mom you were always think its so sweet i cant believe it was a bart of\n",
            "Epoch 84, loss: 0.862\n",
            "Epoch 85. Time: 1.365, Train loss: 0.859\n",
            "youre a great mom you were always think its so sweet i cant believe it was a bart of\n",
            "Epoch 89, loss: 0.846\n",
            "Epoch 90. Time: 1.355, Train loss: 0.843\n",
            "youre a great mom you were always think its so sweet i cant believe it was a bart of\n",
            "Epoch 94, loss: 0.830\n",
            "Epoch 95. Time: 1.349, Train loss: 0.827\n",
            "youre a great mom you were always think its so sweet i cant believe it was a bad as \n",
            "Epoch 99, loss: 0.815\n",
            "Epoch 100. Time: 1.340, Train loss: 0.812\n",
            "youre a great mom you were always think its so sweet and the start of the simpsons w\n",
            "Epoch 104, loss: 0.802\n",
            "Epoch 105. Time: 1.345, Train loss: 0.800\n",
            "youre a great mom you were always think its so sweet and the start of the could i wa\n",
            "Epoch 109, loss: 0.790\n",
            "Epoch 110. Time: 1.340, Train loss: 0.787\n",
            "youre a great mom you were always think the fell shows of the simpsons where did you\n",
            "Epoch 114, loss: 0.776\n",
            "Epoch 115. Time: 1.338, Train loss: 0.773\n",
            "youre a great mom you were always want to go to the story on the way the end of the \n",
            "Epoch 119, loss: 0.762\n",
            "Epoch 120. Time: 1.331, Train loss: 0.760\n",
            "youre a great mom you were always want to go to the story on the back of the test of\n",
            "Epoch 124, loss: 0.750\n",
            "Epoch 125. Time: 1.330, Train loss: 0.747\n",
            "youre a great mom you were always want to go to the story on the back of the test of\n",
            "Epoch 129, loss: 0.736\n",
            "Epoch 130. Time: 1.331, Train loss: 0.734\n",
            "youre a great mom you were always want to go to the story on the back of the truth i\n",
            "Epoch 134, loss: 0.724\n",
            "Epoch 135. Time: 1.335, Train loss: 0.722\n",
            "youre a great mom you were always there we could be a man who ever likes the last ti\n",
            "Epoch 139, loss: 0.715\n",
            "Epoch 140. Time: 1.347, Train loss: 0.713\n",
            "youre a great mom you were always come to the company theyre gonna take the teacher \n",
            "Epoch 144, loss: 0.703\n",
            "Epoch 145. Time: 1.339, Train loss: 0.700\n",
            "youre a great mom you were always college fun thank you should be a hat a trip stop \n",
            "Epoch 149, loss: 0.688\n",
            "Epoch 150. Time: 1.348, Train loss: 0.686\n",
            "youre a great mom you were always college fun thank you should be a hat a trilk of t\n",
            "Epoch 154, loss: 0.675\n",
            "Epoch 155. Time: 1.348, Train loss: 0.673\n",
            "youre a great mom you were always college fun thank you so much dad i have a college\n",
            "Epoch 159, loss: 0.665\n",
            "Epoch 160. Time: 1.353, Train loss: 0.663\n",
            "youre a great mom you were always college everyone who winks me in the first like a \n",
            "Epoch 164, loss: 0.656\n",
            "Epoch 165. Time: 1.349, Train loss: 0.654\n",
            "youre a great mom you were always there were all the time why would you like to be o\n",
            "Epoch 169, loss: 0.645\n",
            "Epoch 170. Time: 1.355, Train loss: 0.642\n",
            "youre a great mom you were always there were gonna to kill the tear bart who doesnt \n",
            "Epoch 174, loss: 0.631\n",
            "Epoch 175. Time: 1.352, Train loss: 0.629\n",
            "youre a great mom you were always think the fure\n",
            "Epoch 179, loss: 0.620\n",
            "Epoch 180. Time: 1.343, Train loss: 0.618\n",
            "youre a great mom you were always think of a bart was so cool that the one who was t\n",
            "Epoch 184, loss: 0.610\n",
            "Epoch 185. Time: 1.353, Train loss: 0.608\n",
            "youre a great mom you were always think of a bart was so cool that is it was meeting\n",
            "Epoch 189, loss: 0.604\n",
            "Epoch 190. Time: 1.348, Train loss: 0.603\n",
            "youre a great mom you were always think im a book in the school but the test of the \n",
            "Epoch 194, loss: 0.602\n",
            "Epoch 195. Time: 1.342, Train loss: 0.600\n",
            "youre a great mom you were always colic ide\n",
            "Epoch 199, loss: 0.587"
          ]
        }
      ]
    }
  ]
}