{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "DLL_HW_13_text_transformer_261021_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dviva1972/denvlaiva/blob/master/DLL_HW_13_text_transformer_261021_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDgFV2qNmj7Z"
      },
      "source": [
        "## DLL\n",
        "\n",
        "## Домашняя работа 13 | Работа с текстом / Трансформеры\n",
        "\n",
        "## Иванов Денис"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfjxV1j_O6Ud"
      },
      "source": [
        "Решить задачу перевода с использованием трансформеров\n",
        "\n",
        "Возьмите англо-испанские пары фраз (www.manythings.org....org/anki/)\n",
        "\n",
        "Обучите на них seq2seq with transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md2vJgynQ3tn"
      },
      "source": [
        "### 1. Импорт библиотек / данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o149LPQazGH3"
      },
      "source": [
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "import torch as tr\n",
        "import torch.nn as nn\n",
        "from   torch import Tensor\n",
        "from   torch.nn import Transformer, TransformerEncoder, TransformerEncoderLayer\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d897g22jzJcd",
        "outputId": "f9bece05-641c-42a9-af47-35bd885816e6"
      },
      "source": [
        "DEVICE = tr.device('cuda:0' if tr.cuda.is_available() else 'cpu')\n",
        "print(f\"work on {(tr.cuda.get_device_name() if DEVICE.type == 'cuda' else 'cpu')}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "work on Tesla P100-PCIE-16GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqBNcHqgTuH2"
      },
      "source": [
        "### 2. Импорт и предобработка текста"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSfLae0XzJfi",
        "outputId": "7b3b177d-9849-4579-a6fd-0e4ed5d17877"
      },
      "source": [
        "!wget https://www.manythings.org/anki/spa-eng.zip\n",
        "!unzip spa-eng.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-26 16:11:23--  https://www.manythings.org/anki/spa-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 172.67.186.54, 104.21.92.44, 2606:4700:3033::ac43:ba36, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|172.67.186.54|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5192744 (5.0M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "spa-eng.zip         100%[===================>]   4.95M  19.1MB/s    in 0.3s    \n",
            "\n",
            "2021-10-26 16:11:23 (19.1 MB/s) - ‘spa-eng.zip’ saved [5192744/5192744]\n",
            "\n",
            "Archive:  spa-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: spa.txt                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Vw1t9mvRG4s"
      },
      "source": [
        "Для решения задачи принят следующий алгоритм подготовки данных:\n",
        "\n",
        "*   в качестве единицы / наблюдения анализа принято 1 предложение, разделенное окончанием пары фраз или знаками окончания предложения '.!?' \n",
        "*   предложение - не более  20 слов, если более - предложение обрезается, к анализу берется 20 первых слов\n",
        "*   в случае, если в исходной паре фраз содержится несколько предложений - то количество наблюдений по паре фраз соответствует количеству отдельных предложений к переводу\n",
        "*   в случае, если в исходной паре фраз количество предложений на входе не совпадает с количеством предложений на выходе - такая пара фраз в обучении модели не участвует\n",
        "*   в случае, если хотя бы в одном из пары предложений встречаются редкие слова (встречающиеся по всему корпусу исходных данных менее 5 раз) - такая пара фраз в обучении модели не участвует"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36CjyvkyzJiC"
      },
      "source": [
        "SRC_LANGUAGE, TGT_LANGUAGE          = 'eng',  'spa'\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {'<unk>': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3}\n",
        "        self.index2word = {0: '<unk>', 1: '<pad>', 2: '<bos>', 3: '<eos>'}\n",
        "        self.word2count = {}\n",
        "        self.n_words = 4 \n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence:#.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8RbtrXjzJkq"
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "           if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Zа-яА-ЯёЁ.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upp4H7rlzJnR"
      },
      "source": [
        "def readLangs(s_limit):\n",
        "    print(\"Reading lines...\")\n",
        "    lines = open('spa.txt', encoding='utf-8').read().strip().split('\\n')\n",
        "    pairs = [[normalizeString(s) for s in re.split('\\t', l)] for l in lines]\n",
        "    pairs_list = []\n",
        "    for string in pairs:\n",
        "        eng_split =  [i for i in re.split('[\\.!?]', string[0]) if i != '']\n",
        "        esp_split =  [i for i in re.split('[\\.!?]', string[1]) if i != '']    \n",
        "        if len(eng_split) ==  len(esp_split):\n",
        "            for i in range(len(eng_split)):            \n",
        "                eng_s = re.findall('[\\w]+', eng_split[i])[:s_limit]\n",
        "                esp_s = re.findall('[\\w]+', esp_split[i])[:s_limit]\n",
        "                pairs_list.append([eng_s, esp_s]) \n",
        "    return pairs_list"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6JKviCpzJqi"
      },
      "source": [
        "def prepareData(i_lang, o_lang, se_limit):\n",
        "    pairs     = readLangs(se_limit)\n",
        "    inp_lang  = Lang(i_lang)\n",
        "    out_lang  = Lang(o_lang)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        inp_lang.addSentence(pair[0])\n",
        "        out_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(inp_lang.name, inp_lang.n_words)\n",
        "    print(out_lang.name, out_lang.n_words)\n",
        "    return inp_lang, out_lang, pairs"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijZ8ShBCzJtc"
      },
      "source": [
        "def Prepare_Filt_Data(im_lang = SRC_LANGUAGE, ou_lang = TGT_LANGUAGE, \n",
        "                      sen_limit = 20,  w_limit = 5):\n",
        "    i1_lang, o1_lang, pairs=prepareData(im_lang, ou_lang, sen_limit)\n",
        "    inp_lang_  = Lang(im_lang)\n",
        "    out_lang_  = Lang(ou_lang)\n",
        "\n",
        "    eng_words_select = set([i for i in i1_lang.word2count.keys() \n",
        "                               if i1_lang.word2count[i] > w_limit])\n",
        "    esp_words_select = set([i for i in o1_lang.word2count.keys() \n",
        "                               if o1_lang.word2count[i] > w_limit])\n",
        "    \n",
        "    for pair in pairs:\n",
        "        for wrd_i in pair[0]:\n",
        "            if wrd_i in eng_words_select:\n",
        "                inp_lang_.addWord(wrd_i)\n",
        "        for wrd_o in pair[1]:\n",
        "            if wrd_o in esp_words_select:\n",
        "                out_lang_.addWord(wrd_o)\n",
        "\n",
        "    print('Using filter: every dict word in sentenses > ', w_limit)\n",
        "    print(inp_lang_.name, inp_lang_.n_words)\n",
        "    print(out_lang_.name, out_lang_.n_words)\n",
        "\n",
        "    pairs_select = []\n",
        "    for i in range(len(pairs)):\n",
        "        if all(word in eng_words_select for word in pairs[i][0]):\n",
        "            if all(word in esp_words_select for word in pairs[i][1]):\n",
        "                   pairs_select.append([pairs[i][0], pairs[i][1]]) \n",
        "\n",
        "    print(len(pairs_select), ' sentenses')\n",
        "    return inp_lang_, out_lang_, pairs_select"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocqhc3f50or3",
        "outputId": "18b610ab-3580-420d-ad8a-a5b43d7cf4e8"
      },
      "source": [
        "input_lang, output_lang, pairs =  Prepare_Filt_Data(sen_limit = 20,  \n",
        "                                                    w_limit   = 5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135511 sentence pairs\n",
            "Trimmed to 135511 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 13490\n",
            "spa 26095\n",
            "Using filter: every dict word in sentenses >  5\n",
            "eng 4999\n",
            "spa 7107\n",
            "101567  sentenses\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC98Mbvna-Kr"
      },
      "source": [
        "full_len   = len(pairs)\n",
        "train_list = list(range(full_len))\n",
        "random.shuffle(train_list)\n",
        "# train_list[:6]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufhP3K7VIiqt"
      },
      "source": [
        "Max_len = 22\n",
        "\n",
        "def sent_to_torch(sent_in, l):\n",
        "    sent_for_torch = np.zeros((Max_len))#.type(tr.long).to(DEVICE)\n",
        "    for b in range(len(sent_in)+2):\n",
        "        if b == 0:\n",
        "            sent_for_torch[b] = 2\n",
        "        elif b <= len(sent_in):\n",
        "            sent_for_torch[b] = l.word2index[sent_in[b-1]]\n",
        "        elif (b==len(sent_in)+1 and b<=Max_len) or b==Max_len+1:\n",
        "            sent_for_torch[b] = 3\n",
        "    return sent_for_torch\n",
        "    \n",
        "# sent_to_torch(pairs[-1][0], input_lang)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_-93vFDgBSp"
      },
      "source": [
        "def torch_to_sent(sent_tens, l):\n",
        "    sentens = [l.index2word(i) for i in sent_tens if i > 3] \n",
        "    return ' '.join(sentens)   "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US0E7znY2t_w"
      },
      "source": [
        "def get_batch(pairs_num_list, batch_size):\n",
        "    batch_list     = random.sample(pairs_num_list, batch_size)\n",
        "\n",
        "    s_np_in  = np.zeros((22))\n",
        "    s_np_out = np.zeros((22))\n",
        "\n",
        "    for a in range(batch_size):\n",
        "\n",
        "        sent_in = pairs[batch_list[a]][0]\n",
        "        sent_in = sent_to_torch(sent_in, input_lang)\n",
        "        s_np_in = np.vstack([s_np_in, sent_in])\n",
        "       \n",
        "        sent_out = pairs[batch_list[a]][1]\n",
        "        sent_out = sent_to_torch(sent_out, output_lang)\n",
        "        s_np_out = np.vstack([s_np_out, sent_out])\n",
        "\n",
        "    data   = tr.tensor( s_np_in[1:], dtype=tr.long, device=DEVICE).view(-1, batch_size)\n",
        "    target = tr.tensor(s_np_out[1:], dtype=tr.long, device=DEVICE).view(-1)\n",
        "\n",
        "    return data, target"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLrWUY2ITyM0"
      },
      "source": [
        "### 3. Архитектура сети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNAHiGpxZkLr"
      },
      "source": [
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, n_tokens_in, n_tokens_out, \n",
        "                 ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        \n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(n_tokens_in, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, n_tokens_out)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (tr.triu(tr.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            device = src.device\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            self.src_mask = mask\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhoUNOCGrWiU"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = tr.zeros(max_len, d_model)\n",
        "        position = tr.arange(0, max_len, dtype=tr.float).unsqueeze(1)\n",
        "        div_term = tr.exp(tr.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = tr.sin(position * div_term)\n",
        "        pe[:, 1::2] = tr.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBw4krOQYkIu"
      },
      "source": [
        "n_tokens_in = input_lang.n_words\n",
        "n_tokens_out= output_lang.n_words"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbrE-uadYkMK"
      },
      "source": [
        "emsize = 200    # embedding dim\n",
        "nhid   = 200    # dim of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2     # the number of TransformerEncoderLayer`s in TransformerEncoder\n",
        "nhead   = 2     # the number of heads in the multiheadattention model\n",
        "dropout = 0.5 \n",
        "model   = TransformerModel(n_tokens_in, n_tokens_out, emsize, \n",
        "                           nhead, nhid, nlayers, dropout).to(DEVICE)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4KtdtWErWig"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5          # learning rate\n",
        "optimizer = tr.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = tr.optim.lr_scheduler.StepLR(optimizer, 3.0, gamma=0.8)\n",
        "batch_size   = 50\n",
        "range_step   = 150\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = n_tokens_out\n",
        "    for i in range(range_step):\n",
        "        data, targets = get_batch(train_list, batch_size)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "        tr.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        if i in range(1, range_step, 20) and i > 0:\n",
        "            cur_loss = total_loss /i\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | lr {:02.2f} | loss {:5.6f} | ppl {:8.6f}'.format(\n",
        "                    epoch, scheduler.get_last_lr()[0],\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "    \n",
        "    return total_loss / range_step, model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLpqBeNArWih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "333316f2-12e8-49a7-b95a-cdfe87e33d13"
      },
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "epochs        = 15\n",
        "best_model    = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    t_loss, model_   = train()\n",
        "    print('-' * 50)\n",
        "    print('|mean loss epoch   {:3}|mean l {:5.6f}|vppl {:4.6f}'.format(epoch, \n",
        "                                                    t_loss, math.exp(t_loss)))\n",
        "    print('-' * 54)\n",
        "\n",
        "    if t_loss         < best_val_loss:\n",
        "        best_val_loss = t_loss\n",
        "        best_model    = model_\n",
        "\n",
        "    scheduler.step()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 | lr 5.00 | loss 13.115902 | ppl 496779.818327\n",
            "| epoch   1 | lr 5.00 | loss 6.752357 | ppl 856.073731\n",
            "| epoch   1 | lr 5.00 | loss 4.912242 | ppl 135.943848\n",
            "| epoch   1 | lr 5.00 | loss 4.199534 | ppl 66.655265\n",
            "| epoch   1 | lr 5.00 | loss 3.766958 | ppl 43.248321\n",
            "| epoch   1 | lr 5.00 | loss 3.492000 | ppl 32.851592\n",
            "| epoch   1 | lr 5.00 | loss 3.319454 | ppl 27.645254\n",
            "| epoch   1 | lr 5.00 | loss 3.183966 | ppl 24.142304\n",
            "--------------------------------------------------\n",
            "|mean loss epoch     1|mean l 3.112020|vppl 22.466376\n",
            "------------------------------------------------------\n",
            "| epoch   2 | lr 5.00 | loss 4.455430 | ppl 86.093186\n",
            "| epoch   2 | lr 5.00 | loss 2.328672 | ppl 10.264298\n",
            "| epoch   2 | lr 5.00 | loss 2.190367 | ppl 8.938496\n",
            "| epoch   2 | lr 5.00 | loss 2.172541 | ppl 8.780570\n",
            "| epoch   2 | lr 5.00 | loss 2.138202 | ppl 8.484172\n",
            "| epoch   2 | lr 5.00 | loss 2.134357 | ppl 8.451610\n",
            "| epoch   2 | lr 5.00 | loss 2.109135 | ppl 8.241106\n",
            "| epoch   2 | lr 5.00 | loss 2.107395 | ppl 8.226782\n",
            "--------------------------------------------------\n",
            "|mean loss epoch     2|mean l 2.081341|vppl 8.015207\n",
            "------------------------------------------------------\n",
            "| epoch   3 | lr 5.00 | loss 3.522746 | ppl 33.877343\n",
            "| epoch   3 | lr 5.00 | loss 1.989352 | ppl 7.310792\n",
            "| epoch   3 | lr 5.00 | loss 1.981649 | ppl 7.254698\n",
            "| epoch   3 | lr 5.00 | loss 1.979987 | ppl 7.242646\n",
            "| epoch   3 | lr 5.00 | loss 1.964298 | ppl 7.129907\n",
            "| epoch   3 | lr 5.00 | loss 1.959246 | ppl 7.093976\n",
            "| epoch   3 | lr 5.00 | loss 1.965564 | ppl 7.138939\n",
            "| epoch   3 | lr 5.00 | loss 1.955477 | ppl 7.067290\n",
            "--------------------------------------------------\n",
            "|mean loss epoch     3|mean l 1.936314|vppl 6.933145\n",
            "------------------------------------------------------\n",
            "| epoch   4 | lr 4.00 | loss 3.817338 | ppl 45.482993\n",
            "| epoch   4 | lr 4.00 | loss 1.943597 | ppl 6.983824\n",
            "| epoch   4 | lr 4.00 | loss 1.876983 | ppl 6.533763\n",
            "| epoch   4 | lr 4.00 | loss 1.862489 | ppl 6.439744\n",
            "| epoch   4 | lr 4.00 | loss 1.853904 | ppl 6.384697\n",
            "| epoch   4 | lr 4.00 | loss 1.847304 | ppl 6.342698\n",
            "| epoch   4 | lr 4.00 | loss 1.856038 | ppl 6.398339\n",
            "| epoch   4 | lr 4.00 | loss 1.857073 | ppl 6.404964\n",
            "--------------------------------------------------\n",
            "|mean loss epoch     4|mean l 1.842937|vppl 6.315057\n",
            "------------------------------------------------------\n",
            "| epoch   5 | lr 4.00 | loss 3.436444 | ppl 31.076241\n",
            "| epoch   5 | lr 4.00 | loss 1.880368 | ppl 6.555918\n",
            "| epoch   5 | lr 4.00 | loss 1.858949 | ppl 6.416987\n",
            "| epoch   5 | lr 4.00 | loss 1.843072 | ppl 6.315909\n",
            "| epoch   5 | lr 4.00 | loss 1.845334 | ppl 6.330215\n",
            "| epoch   5 | lr 4.00 | loss 1.843125 | ppl 6.316244\n",
            "| epoch   5 | lr 4.00 | loss 1.838740 | ppl 6.288608\n",
            "| epoch   5 | lr 4.00 | loss 1.835642 | ppl 6.269156\n",
            "--------------------------------------------------\n",
            "|mean loss epoch     5|mean l 1.822879|vppl 6.189653\n",
            "------------------------------------------------------\n",
            "| epoch   6 | lr 4.00 | loss 3.584946 | ppl 36.051404\n",
            "| epoch   6 | lr 4.00 | loss 1.906547 | ppl 6.729812\n",
            "| epoch   6 | lr 4.00 | loss 1.873761 | ppl 6.512745\n",
            "| epoch   6 | lr 4.00 | loss 1.837795 | ppl 6.282670\n",
            "| epoch   6 | lr 4.00 | loss 1.813406 | ppl 6.131296\n",
            "| epoch   6 | lr 4.00 | loss 1.814219 | ppl 6.136283\n",
            "| epoch   6 | lr 4.00 | loss 1.817976 | ppl 6.159380\n",
            "| epoch   6 | lr 4.00 | loss 1.834014 | ppl 6.258957\n",
            "--------------------------------------------------\n",
            "|mean loss epoch     6|mean l 1.818560|vppl 6.162975\n",
            "------------------------------------------------------\n",
            "| epoch   7 | lr 3.20 | loss 3.368636 | ppl 29.038877\n",
            "| epoch   7 | lr 3.20 | loss 1.886737 | ppl 6.597804\n",
            "| epoch   7 | lr 3.20 | loss 1.847308 | ppl 6.342722\n",
            "| epoch   7 | lr 3.20 | loss 1.810303 | ppl 6.112300\n",
            "| epoch   7 | lr 3.20 | loss 1.794325 | ppl 6.015412\n",
            "| epoch   7 | lr 3.20 | loss 1.795929 | ppl 6.025071\n",
            "| epoch   7 | lr 3.20 | loss 1.806226 | ppl 6.087429\n",
            "| epoch   7 | lr 3.20 | loss 1.795883 | ppl 6.024792\n",
            "--------------------------------------------------\n",
            "|mean loss epoch     7|mean l 1.783950|vppl 5.953323\n",
            "------------------------------------------------------\n",
            "| epoch   8 | lr 3.20 | loss 3.530221 | ppl 34.131517\n",
            "| epoch   8 | lr 3.20 | loss 1.789362 | ppl 5.985631\n",
            "| epoch   8 | lr 3.20 | loss 1.774319 | ppl 5.896262\n",
            "| epoch   8 | lr 3.20 | loss 1.756657 | ppl 5.793039\n",
            "| epoch   8 | lr 3.20 | loss 1.749105 | ppl 5.749455\n",
            "| epoch   8 | lr 3.20 | loss 1.753219 | ppl 5.773155\n",
            "| epoch   8 | lr 3.20 | loss 1.755518 | ppl 5.786442\n",
            "| epoch   8 | lr 3.20 | loss 1.750957 | ppl 5.760114\n",
            "--------------------------------------------------\n",
            "|mean loss epoch     8|mean l 1.741055|vppl 5.703356\n",
            "------------------------------------------------------\n",
            "| epoch   9 | lr 3.20 | loss 3.224778 | ppl 25.147986\n",
            "| epoch   9 | lr 3.20 | loss 1.794407 | ppl 6.015908\n",
            "| epoch   9 | lr 3.20 | loss 1.744905 | ppl 5.725360\n",
            "| epoch   9 | lr 3.20 | loss 1.727033 | ppl 5.623943\n",
            "| epoch   9 | lr 3.20 | loss 1.725820 | ppl 5.617128\n",
            "| epoch   9 | lr 3.20 | loss 1.720219 | ppl 5.585750\n",
            "| epoch   9 | lr 3.20 | loss 1.725923 | ppl 5.617703\n",
            "| epoch   9 | lr 3.20 | loss 1.722555 | ppl 5.598815\n",
            "--------------------------------------------------\n",
            "|mean loss epoch     9|mean l 1.710759|vppl 5.533158\n",
            "------------------------------------------------------\n",
            "| epoch  10 | lr 2.56 | loss 3.617875 | ppl 37.258309\n",
            "| epoch  10 | lr 2.56 | loss 1.806351 | ppl 6.088191\n",
            "| epoch  10 | lr 2.56 | loss 1.769427 | ppl 5.867490\n",
            "| epoch  10 | lr 2.56 | loss 1.737373 | ppl 5.682393\n",
            "| epoch  10 | lr 2.56 | loss 1.729455 | ppl 5.637582\n",
            "| epoch  10 | lr 2.56 | loss 1.718711 | ppl 5.577332\n",
            "| epoch  10 | lr 2.56 | loss 1.713584 | ppl 5.548812\n",
            "| epoch  10 | lr 2.56 | loss 1.713190 | ppl 5.546627\n",
            "--------------------------------------------------\n",
            "|mean loss epoch    10|mean l 1.698060|vppl 5.463340\n",
            "------------------------------------------------------\n",
            "| epoch  11 | lr 2.56 | loss 3.345375 | ppl 28.371215\n",
            "| epoch  11 | lr 2.56 | loss 1.776087 | ppl 5.906697\n",
            "| epoch  11 | lr 2.56 | loss 1.738755 | ppl 5.690255\n",
            "| epoch  11 | lr 2.56 | loss 1.718198 | ppl 5.574476\n",
            "| epoch  11 | lr 2.56 | loss 1.704547 | ppl 5.498895\n",
            "| epoch  11 | lr 2.56 | loss 1.696045 | ppl 5.452340\n",
            "| epoch  11 | lr 2.56 | loss 1.691565 | ppl 5.427970\n",
            "| epoch  11 | lr 2.56 | loss 1.695718 | ppl 5.450556\n",
            "--------------------------------------------------\n",
            "|mean loss epoch    11|mean l 1.681869|vppl 5.375596\n",
            "------------------------------------------------------\n",
            "| epoch  12 | lr 2.56 | loss 3.236867 | ppl 25.453842\n",
            "| epoch  12 | lr 2.56 | loss 1.755722 | ppl 5.787624\n",
            "| epoch  12 | lr 2.56 | loss 1.709000 | ppl 5.523433\n",
            "| epoch  12 | lr 2.56 | loss 1.701175 | ppl 5.480385\n",
            "| epoch  12 | lr 2.56 | loss 1.701049 | ppl 5.479692\n",
            "| epoch  12 | lr 2.56 | loss 1.690059 | ppl 5.419800\n",
            "| epoch  12 | lr 2.56 | loss 1.690976 | ppl 5.424773\n",
            "| epoch  12 | lr 2.56 | loss 1.687455 | ppl 5.405708\n",
            "--------------------------------------------------\n",
            "|mean loss epoch    12|mean l 1.675283|vppl 5.340307\n",
            "------------------------------------------------------\n",
            "| epoch  13 | lr 2.05 | loss 3.177316 | ppl 23.982297\n",
            "| epoch  13 | lr 2.05 | loss 1.766065 | ppl 5.847800\n",
            "| epoch  13 | lr 2.05 | loss 1.725343 | ppl 5.614449\n",
            "| epoch  13 | lr 2.05 | loss 1.711354 | ppl 5.536454\n",
            "| epoch  13 | lr 2.05 | loss 1.704075 | ppl 5.496300\n",
            "| epoch  13 | lr 2.05 | loss 1.694038 | ppl 5.441409\n",
            "| epoch  13 | lr 2.05 | loss 1.683593 | ppl 5.384872\n",
            "| epoch  13 | lr 2.05 | loss 1.685210 | ppl 5.393583\n",
            "--------------------------------------------------\n",
            "|mean loss epoch    13|mean l 1.676093|vppl 5.344635\n",
            "------------------------------------------------------\n",
            "| epoch  14 | lr 2.05 | loss 3.408230 | ppl 30.211735\n",
            "| epoch  14 | lr 2.05 | loss 1.769985 | ppl 5.870766\n",
            "| epoch  14 | lr 2.05 | loss 1.710550 | ppl 5.532002\n",
            "| epoch  14 | lr 2.05 | loss 1.703744 | ppl 5.494482\n",
            "| epoch  14 | lr 2.05 | loss 1.699231 | ppl 5.469740\n",
            "| epoch  14 | lr 2.05 | loss 1.703489 | ppl 5.493081\n",
            "| epoch  14 | lr 2.05 | loss 1.697857 | ppl 5.462229\n",
            "| epoch  14 | lr 2.05 | loss 1.694865 | ppl 5.445909\n",
            "--------------------------------------------------\n",
            "|mean loss epoch    14|mean l 1.683921|vppl 5.386634\n",
            "------------------------------------------------------\n",
            "| epoch  15 | lr 2.05 | loss 3.251341 | ppl 25.824948\n",
            "| epoch  15 | lr 2.05 | loss 1.769224 | ppl 5.866299\n",
            "| epoch  15 | lr 2.05 | loss 1.711865 | ppl 5.539280\n",
            "| epoch  15 | lr 2.05 | loss 1.702982 | ppl 5.490293\n",
            "| epoch  15 | lr 2.05 | loss 1.684785 | ppl 5.391290\n",
            "| epoch  15 | lr 2.05 | loss 1.676390 | ppl 5.346221\n",
            "| epoch  15 | lr 2.05 | loss 1.675179 | ppl 5.339751\n",
            "| epoch  15 | lr 2.05 | loss 1.671811 | ppl 5.321797\n",
            "--------------------------------------------------\n",
            "|mean loss epoch    15|mean l 1.661002|vppl 5.264584\n",
            "------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPhq91sXrWii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6ace509-f786-40c8-8ab6-012180b651d3"
      },
      "source": [
        "best_model"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerModel(\n",
              "  (pos_encoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
              "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.5, inplace=False)\n",
              "        (dropout2): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (1): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
              "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.5, inplace=False)\n",
              "        (dropout2): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (encoder): Embedding(4999, 200)\n",
              "  (decoder): Linear(in_features=200, out_features=7107, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1S-yz6myxq7"
      },
      "source": [
        "def translate_to_spain(idx):\n",
        "    print('Предложение на английском :', ' '.join(pairs[idx][0])) \n",
        "    print('Предложение на испанском  :', ' '.join(pairs[idx][1])) \n",
        "\n",
        "    sent_np_in = sent_to_torch(pairs[idx][0], input_lang)\n",
        "    output     = best_model(tr.tensor(sent_np_in,  dtype=tr.long, device=DEVICE))\n",
        "\n",
        "    print('Перевод seq2seq           :', \n",
        "          ' '.join([output_lang.index2word[i.item()] \n",
        "                    for i in output[0].data.topk(1)[1] if i > 3])) "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2Aq6LgAVXzY",
        "outputId": "0ae69b30-4c04-46d6-f983-26b0c753cf8a"
      },
      "source": [
        "translate_to_spain(82368)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предложение на английском : tom wanted mary to loan him some money\n",
            "Предложение на испанском  : tom queria que maria le prestara dinero\n",
            "Перевод seq2seq           : tom queria mary a dinero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDtUQ6NHWSYt",
        "outputId": "c843b2c9-a9c1-4ebd-e675-70985a6ef2fc"
      },
      "source": [
        "translate_to_spain(8207)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предложение на английском : she isn t young\n",
            "Предложение на испанском  : ella no es joven\n",
            "Перевод seq2seq           : ella no no joven\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG28ArVUWdOI",
        "outputId": "96450d87-632a-4425-ecc6-994e60b18f66"
      },
      "source": [
        "translate_to_spain(8231)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предложение на английском : she s my sister\n",
            "Предложение на испанском  : ella es mi hermana\n",
            "Перевод seq2seq           : ella es mi hermana\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTH3w9o_Xrgl",
        "outputId": "c9f28cac-16de-4c89-f9b0-03038a4e4028"
      },
      "source": [
        "translate_to_spain(55770)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предложение на английском : he looks just like his mother\n",
            "Предложение на испанском  : el se parece a su madre\n",
            "Перевод seq2seq           : el no su madre\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjefxnqhVvgZ"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}