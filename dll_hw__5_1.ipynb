{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dll_hw__5_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNq5KGf/RWW3FEttWlRnQLw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dviva1972/denvlaiva/blob/master/dll_hw__5_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gnr5b_WvX7m"
      },
      "source": [
        "## DLL\n",
        "\n",
        "## Домашняя работа 5  |  RNN\n",
        "## Иванов Денис"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwq0jG2ZuDrZ"
      },
      "source": [
        "## Задание 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR0L_WYruLAs"
      },
      "source": [
        "1. Напишите алгоритм шифра цезаря для генерации выборки \n",
        "\n",
        "2.  Создайте и обучите нейронную сеть (вход - зашифрованная фраза, выход - дешифрованная фраза)\n",
        "\n",
        "3. Проверьте качество модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Rq-LP3YuI2w"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAnYff42v4TQ"
      },
      "source": [
        "1.Алгоритм шифра Цезаря "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeaiOWUIuPc5"
      },
      "source": [
        "def Сaesar(string, num):\n",
        "    output = ''\n",
        "    for c in string:\n",
        "        if c.isalpha():\n",
        "            new_num = ord(c) + num\n",
        "            if new_num > ord('z'):\n",
        "                new_num -= 26\n",
        "            output += chr(new_num)\n",
        "        else:\n",
        "            output += c\n",
        "    return output"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8K4bw1OBuVUG",
        "outputId": "1a6cb0d8-45c9-443e-d116-90642c5d255d"
      },
      "source": [
        "Сaesar('trew', 2)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'vtgy'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-Wm73XEwZVd"
      },
      "source": [
        "Сгенерим данные на базе изречений симпсонов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AObYk7DbovUs"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "from google.colab import drive "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9--cCzKKGzY"
      },
      "source": [
        "import copy"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLDPN3VXJsLJ"
      },
      "source": [
        "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkKa2S1so4jE",
        "outputId": "7d60c45f-85ce-49d2-e664-c145794990d6"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "zE4mhjfquf-m",
        "outputId": "7ea1823e-0a48-4279-bedf-a7cfdb25cec4"
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/DLL/simpsons/data.csv', encoding='utf-8-sig').iloc[:,[-2]]\n",
        "df = df.dropna(subset=['normalized_text'])\n",
        "\n",
        "df['shift']   = [random.randint(1, 10) for i in range(len(df))]\n",
        "df['text_in'] = [' '.join(re.findall('[\\w]+', i)) for i in df['normalized_text']]\n",
        "df['text_out']= df.loc[:, ['text_in', 'shift']].apply(\n",
        "                        lambda row: Сaesar(row['text_in'], row['shift']), axis=1)\n",
        "\n",
        "df = df.iloc[:,1:]\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>shift</th>\n",
              "      <th>text_in</th>\n",
              "      <th>text_out</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9</td>\n",
              "      <td>maggie look whats that</td>\n",
              "      <td>vjpprn uxxt fqjcb cqjc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>lee mur lee mur</td>\n",
              "      <td>pii qyv pii qyv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>zee boo zee boo</td>\n",
              "      <td>fkk huu fkk huu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>im trying to teach maggie that nature doesnt e...</td>\n",
              "      <td>ko vtakpi vq vgcej ociikg vjcv pcvwtg fqgupv g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>its like an ox only it has a hump and a dewlap...</td>\n",
              "      <td>kvu nkmg cp qz qpna kv jcu c jwor cpf c fgyncr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   shift  ...                                           text_out\n",
              "0      9  ...                             vjpprn uxxt fqjcb cqjc\n",
              "1      4  ...                                    pii qyv pii qyv\n",
              "2      6  ...                                    fkk huu fkk huu\n",
              "3      2  ...  ko vtakpi vq vgcej ociikg vjcv pcvwtg fqgupv g...\n",
              "4      2  ...  kvu nkmg cp qz qpna kv jcu c jwor cpf c fgyncr...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ewjerm-mbyco",
        "outputId": "d98c4342-4c47-414f-d1ac-6f7e4afe1117"
      },
      "source": [
        "[[c for c in ph] for ph in df.text_in if type(ph) is not str]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAhwTuOUbUfs"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQd6ZaqzavkE"
      },
      "source": [
        "train, test = train_test_split(df, test_size=0.2)\n",
        "train_text  = train['text_out' ].tolist()\n",
        "train_label = train['text_in'].tolist()\n",
        "test_text   = test[ 'text_out' ].tolist()\n",
        "test_label  = test[ 'text_in'].tolist()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gsSpu3SOVsa"
      },
      "source": [
        "INDEX_TO_CHAR = ['none'] + [w for w in set('abcdefghijklmnopqrstuvwxyz ')]\n",
        "CHAR_TO_INDEX = {w: i for i, w in enumerate(INDEX_TO_CHAR)}\n",
        "\n",
        "MAX_LEN       = 20\n",
        "\n",
        "def convert_to_torch(text):\n",
        "    output = torch.zeros((len(text), MAX_LEN), dtype=int)\n",
        "    for i in range(len(text)):\n",
        "        for j, w in enumerate(text[i]):\n",
        "            if j >= MAX_LEN:\n",
        "                break\n",
        "            output[i, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX['none'])\n",
        "\n",
        "    return output\n",
        "\n",
        "X_train= convert_to_torch(train_text)\n",
        "Y_train= convert_to_torch(train_label)\n",
        "X_test = convert_to_torch(test_text)\n",
        "Y_test = convert_to_torch(test_label)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm2UTy-eavqU"
      },
      "source": [
        "class RNN_Network(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNN_Network, self).__init__()\n",
        "        self.embeddings = torch.nn.Embedding(len(INDEX_TO_CHAR), 28)\n",
        "        self.rnn = torch.nn.RNN(28, 256, batch_first=True)\n",
        "        self.linear = torch.nn.Linear(256, 28)\n",
        "\n",
        "    def forward(self, sentences, state=None):\n",
        "        embds = self.embeddings(sentences)\n",
        "        out, new_state = self.rnn(embds, state)\n",
        "        result = self.linear(out)\n",
        "        return result, new_state"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8zf6Spu8kxd"
      },
      "source": [
        "model = RNN_Network()\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
        "epochs = 20"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdSOS6x7WEFD",
        "outputId": "b2ec34f2-4690-4ddd-a9bd-b9f7a4be35d8"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    train_loss = 0.\n",
        "    train_passed = 0\n",
        "    test_loss = 0\n",
        "    test_passed = 0\n",
        "\n",
        "    for i in range(int(len(X_train) / 100)):\n",
        "        X_batch = X_train[i * 100:(i + 1) * 100]\n",
        "        Y_batch = Y_train[i * 100:(i + 1) * 100].flatten()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        answers, _ = model.forward(X_batch)\n",
        "        answers = answers.view(-1, len(INDEX_TO_CHAR))\n",
        "        loss = criterion(answers, Y_batch)\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_passed += 1\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        answers, _ = model.forward(X_test)\n",
        "        answers = answers.view(-1, len(INDEX_TO_CHAR))\n",
        "        loss = criterion(answers, Y_test.flatten())\n",
        "        test_loss += loss.item()\n",
        "        test_passed += 1\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}. Time: {time.time() - start:.3f}, Train loss: {train_loss / train_passed:.3f}, Test loss: {test_loss / test_passed:.6f}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1. Time: 2.948, Train loss: 1.687, Test loss: 1.284145\n",
            "Epoch 2. Time: 2.604, Train loss: 1.254, Test loss: 1.181833\n",
            "Epoch 3. Time: 2.621, Train loss: 1.162, Test loss: 1.090840\n",
            "Epoch 4. Time: 2.578, Train loss: 1.034, Test loss: 0.934189\n",
            "Epoch 5. Time: 2.684, Train loss: 0.856, Test loss: 0.778867\n",
            "Epoch 6. Time: 2.605, Train loss: 0.716, Test loss: 0.700779\n",
            "Epoch 7. Time: 2.629, Train loss: 0.596, Test loss: 0.603406\n",
            "Epoch 8. Time: 2.577, Train loss: 0.514, Test loss: 0.567529\n",
            "Epoch 9. Time: 2.573, Train loss: 0.454, Test loss: 0.494375\n",
            "Epoch 10. Time: 2.626, Train loss: 0.408, Test loss: 0.473532\n",
            "Epoch 11. Time: 2.587, Train loss: 0.386, Test loss: 0.419784\n",
            "Epoch 12. Time: 2.589, Train loss: 0.349, Test loss: 0.417215\n",
            "Epoch 13. Time: 2.603, Train loss: 0.301, Test loss: 0.367100\n",
            "Epoch 14. Time: 2.609, Train loss: 0.267, Test loss: 0.344502\n",
            "Epoch 15. Time: 2.533, Train loss: 0.245, Test loss: 0.334135\n",
            "Epoch 16. Time: 2.618, Train loss: 0.234, Test loss: 0.328219\n",
            "Epoch 17. Time: 2.654, Train loss: 0.231, Test loss: 0.339054\n",
            "Epoch 18. Time: 2.621, Train loss: 0.218, Test loss: 0.320837\n",
            "Epoch 19. Time: 2.604, Train loss: 0.201, Test loss: 0.318068\n",
            "Epoch 20. Time: 2.598, Train loss: 0.235, Test loss: 0.319424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU5xOobu1nt3"
      },
      "source": [
        "###Задание 2.\n",
        "\n",
        "На основе лекционного ноутбука.\n",
        "\n",
        "а) построить RNN-ячейку на основе полносвязных слоев\n",
        "\n",
        "б) применить построенную ячейку для генерации текста с выражениями героев сериала “Симпсоны”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpwH3UmfhHJb"
      },
      "source": [
        "X = convert_to_torch(df['text_in'].tolist())\n",
        "\n",
        "MAX_LEN = 50"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwuCu3Pweaqj"
      },
      "source": [
        "class Network(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.embed = torch.nn.Embedding(len(CHAR_TO_INDEX), 28)\n",
        "        self.rnn = torch.nn.RNN(28, 812, batch_first=True)\n",
        "        self.linear = torch.nn.Linear(812, len(INDEX_TO_CHAR))\n",
        "        \n",
        "    def forward(self, sentences, state=None):\n",
        "        embed = self.embed(sentences)\n",
        "        o, s = self.rnn(embed)\n",
        "        out = self.linear(o)\n",
        "        return out"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm007vK-cKO3"
      },
      "source": [
        "def generate_sentence(txt_list):\n",
        "\n",
        "    sentence =  [c for c in txt_list]\n",
        "    x = torch.zeros((1, len(sentence)), dtype=int).to(dev)\n",
        "    \n",
        "    for j,w in enumerate(sentence):\n",
        "        if j>=MAX_LEN:\n",
        "            break\n",
        "        x[0, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX['none'])\n",
        "        \n",
        "    for i in range(MAX_LEN):\n",
        "        o = model(x)\n",
        "        w = torch.argmax(o[-1, -1, :], keepdim=True)\n",
        "        x = torch.cat([x, w.unsqueeze(0).unsqueeze(1)], axis=1)\n",
        "        ww = INDEX_TO_CHAR[w]\n",
        "        if ww == 'none':\n",
        "            break\n",
        "\n",
        "        sentence.append(ww)\n",
        "\n",
        "    return ''.join(sentence)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSKTInTfeBZY"
      },
      "source": [
        "model     = Network().to(dev)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=.0001)\n",
        "\n",
        "phrase = 'youre a great mom you were always '"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bqFARFzcKSS",
        "outputId": "4007bd2d-2423-41a2-a73a-4b52250a783e"
      },
      "source": [
        "for ep in range(200):\n",
        "    start = time.time()\n",
        "    train_loss = 0.\n",
        "    train_passed = 0\n",
        "\n",
        "    for i in range(int(len(X) / 100)):\n",
        "        batch = X[i * 100:(i + 1) * 100] \n",
        "        X_batch = batch[:, :-1].to(dev)\n",
        "        Y_batch = batch[:, 1:].flatten().to(dev) \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        answers = model(X_batch)\n",
        "        answers = answers.view(-1, len(INDEX_TO_CHAR))\n",
        "        loss = criterion(answers, Y_batch).to(dev)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_passed += 1     \n",
        "\n",
        "    if ep%5 == 0: \n",
        "        print(\"\\nEpoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))\n",
        "        s = generate_sentence(phrase)\n",
        "        print(s)\n",
        "    else:\n",
        "        print(f\"\\rEpoch {ep}, loss: {train_loss / train_passed:.3f}\", end='') "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 0. Time: 1.396, Train loss: 2.577\n",
            "youre a great mom you were always io the  ao the  ao the  ao the  ao the  ao the  ao\n",
            "Epoch 4, loss: 1.930\n",
            "Epoch 5. Time: 1.159, Train loss: 1.886\n",
            "youre a great mom you were always i gont in the sont in the sont in the sont in the \n",
            "Epoch 9, loss: 1.755\n",
            "Epoch 10. Time: 1.162, Train loss: 1.728\n",
            "youre a great mom you were always are the sont the sont the sont the sont the sont t\n",
            "Epoch 14, loss: 1.633\n",
            "Epoch 15. Time: 1.151, Train loss: 1.613\n",
            "youre a great mom you were always a mant the some the some the some the some the som\n",
            "Epoch 19, loss: 1.539\n",
            "Epoch 20. Time: 1.156, Train loss: 1.522\n",
            "youre a great mom you were always dont want to the some on the bart the really sore \n",
            "Epoch 24, loss: 1.463\n",
            "Epoch 25. Time: 1.153, Train loss: 1.450\n",
            "youre a great mom you were always dad the sere stop the bart the really sore what ar\n",
            "Epoch 29, loss: 1.401\n",
            "Epoch 30. Time: 1.151, Train loss: 1.390\n",
            "youre a great mom you were always the list sare i cant believe the bart i have to se\n",
            "Epoch 34, loss: 1.349\n",
            "Epoch 35. Time: 1.151, Train loss: 1.339\n",
            "youre a great mom you were always the list sare i cant believe it was the list sare \n",
            "Epoch 39, loss: 1.303\n",
            "Epoch 40. Time: 1.154, Train loss: 1.295\n",
            "youre a great mom you were always the list sare i cant believe it was the list sare \n",
            "Epoch 44, loss: 1.263\n",
            "Epoch 45. Time: 1.161, Train loss: 1.255\n",
            "youre a great mom you were always the list simpson what are you do you think its a m\n",
            "Epoch 49, loss: 1.225\n",
            "Epoch 50. Time: 1.158, Train loss: 1.218\n",
            "youre a great mom you were always the take it was a bart i have a man a bat stop the\n",
            "Epoch 54, loss: 1.191\n",
            "Epoch 55. Time: 1.155, Train loss: 1.184\n",
            "youre a great mom you were always the take it was a bart in the bart is the best hav\n",
            "Epoch 59, loss: 1.158\n",
            "Epoch 60. Time: 1.155, Train loss: 1.152\n",
            "youre a great mom you were always the take it was a bart in the bart is the best hav\n",
            "Epoch 64, loss: 1.127\n",
            "Epoch 65. Time: 1.157, Train loss: 1.121\n",
            "youre a great mom you were always the take it was a bart in the bart is the best hav\n",
            "Epoch 69, loss: 1.097\n",
            "Epoch 70. Time: 1.161, Train loss: 1.091\n",
            "youre a great mom you were always the tame i have to see that mom i steal youre gonn\n",
            "Epoch 74, loss: 1.068\n",
            "Epoch 75. Time: 1.148, Train loss: 1.062\n",
            "youre a great mom you were always the tarting to the pain it out fart\n",
            "Epoch 79, loss: 1.040\n",
            "Epoch 80. Time: 1.152, Train loss: 1.034\n",
            "youre a great mom you were always the tarting to the poor the bart i cant believe in\n",
            "Epoch 84, loss: 1.014\n",
            "Epoch 85. Time: 1.154, Train loss: 1.009\n",
            "youre a great mom you were always the tart i have to see that mom i steal ok a nawer\n",
            "Epoch 89, loss: 0.989\n",
            "Epoch 90. Time: 1.155, Train loss: 0.984\n",
            "youre a great mom you were always the time i have to starting to the past be bobor o\n",
            "Epoch 94, loss: 0.963\n",
            "Epoch 95. Time: 1.153, Train loss: 0.958\n",
            "youre a great mom you were always the time i have to ster is becouse the poor is som\n",
            "Epoch 99, loss: 0.937\n",
            "Epoch 100. Time: 1.166, Train loss: 0.932\n",
            "youre a great mom you were always the time i have to starting to the past bar she li\n",
            "Epoch 104, loss: 0.912\n",
            "Epoch 105. Time: 1.151, Train loss: 0.908\n",
            "youre a great mom you were always the time i have to starting to the past bar sand s\n",
            "Epoch 109, loss: 0.889\n",
            "Epoch 110. Time: 1.155, Train loss: 0.885\n",
            "youre a great mom you were always the time i had to take me wout becouse theyre all \n",
            "Epoch 114, loss: 0.870\n",
            "Epoch 115. Time: 1.155, Train loss: 0.866\n",
            "youre a great mom you were always the times why are you so happy you got the moet sh\n",
            "Epoch 119, loss: 0.851\n",
            "Epoch 120. Time: 1.156, Train loss: 0.848\n",
            "youre a great mom you were always the time i hadnt manay an i cant believe ive been \n",
            "Epoch 124, loss: 0.832\n",
            "Epoch 125. Time: 1.159, Train loss: 0.828\n",
            "youre a great mom you were always they do not really have to deterning the loses the\n",
            "Epoch 129, loss: 0.812\n",
            "Epoch 130. Time: 1.160, Train loss: 0.808\n",
            "youre a great mom you were always they do not that do you think youll bart a shore o\n",
            "Epoch 134, loss: 0.795\n",
            "Epoch 135. Time: 1.154, Train loss: 0.792\n",
            "youre a great mom you were always they do not that do you think you\n",
            "Epoch 139, loss: 0.780\n",
            "Epoch 140. Time: 1.158, Train loss: 0.777\n",
            "youre a great mom you were always they do not gonna be so gone tine ofr happy to the\n",
            "Epoch 144, loss: 0.764\n",
            "Epoch 145. Time: 1.154, Train loss: 0.761\n",
            "youre a great mom you were always they do no hir the gone you being i think youre br\n",
            "Epoch 149, loss: 0.746\n",
            "Epoch 150. Time: 1.167, Train loss: 0.742\n",
            "youre a great mom you were always they do no hir the gone you been everything i beli\n",
            "Epoch 154, loss: 0.728\n",
            "Epoch 155. Time: 1.151, Train loss: 0.725\n",
            "youre a great mom you were always they do no showld we do the moter sad\n",
            "Epoch 159, loss: 0.711\n",
            "Epoch 160. Time: 1.156, Train loss: 0.708\n",
            "youre a great mom you were always they do no shest please mom\n",
            "Epoch 164, loss: 0.696\n",
            "Epoch 165. Time: 1.154, Train loss: 0.692\n",
            "youre a great mom you were always they do ma boo\n",
            "Epoch 169, loss: 0.679\n",
            "Epoch 170. Time: 1.157, Train loss: 0.676\n",
            "youre a great mom you were always they do no shest please make this beanty bart i to\n",
            "Epoch 174, loss: 0.662\n",
            "Epoch 175. Time: 1.156, Train loss: 0.659\n",
            "youre a great mom you were always the tarting to the romour sone\n",
            "Epoch 179, loss: 0.647\n",
            "Epoch 180. Time: 1.156, Train loss: 0.644\n",
            "youre a great mom you were always the tarting to the romour sone\n",
            "Epoch 184, loss: 0.633\n",
            "Epoch 185. Time: 1.158, Train loss: 0.630\n",
            "youre a great mom you were always the tarting to the romet in\n",
            "Epoch 189, loss: 0.621\n",
            "Epoch 190. Time: 1.153, Train loss: 0.619\n",
            "youre a great mom you were always the cart of you stincanna cancop se bated fello\n",
            "Epoch 194, loss: 0.611\n",
            "Epoch 195. Time: 1.157, Train loss: 0.610\n",
            "youre a great mom you were always n verver the most comfort\n",
            "Epoch 199, loss: 0.604"
          ]
        }
      ]
    }
  ]
}